{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시퀀스-투-시퀀스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq는 크게 인코더와 디코더 두 개의 모듈로 구성\n",
    "# 입력: 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만든다. (컨텍스트 벡터)\n",
    "# 인코더: 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송\n",
    "# 디코더: 번역"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 레벨 기계 번역기 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq -> 훈련 데이터로 병렬 코퍼스(두 개 이상의 언어가 병렬적으로 구성된 코퍼스)가 필요\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib3\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 197463\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv('./fra-eng/fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수 :',len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33710</th>\n",
       "      <td>Yeah, you're right.</td>\n",
       "      <td>Ouais, vous avez raison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Taste it.</td>\n",
       "      <td>Goûte-la.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43841</th>\n",
       "      <td>I'm your best friend.</td>\n",
       "      <td>Je suis votre meilleur ami.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>I work alone.</td>\n",
       "      <td>Je travaille seule.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22368</th>\n",
       "      <td>You're fortunate.</td>\n",
       "      <td>Vous êtes chanceuse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18430</th>\n",
       "      <td>I am pretty sure.</td>\n",
       "      <td>Je suis relativement sûr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41100</th>\n",
       "      <td>Do you need the keys?</td>\n",
       "      <td>T'as besoin des clefs ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48391</th>\n",
       "      <td>Don't throw that away.</td>\n",
       "      <td>Ne le jetez pas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>I've no idea.</td>\n",
       "      <td>Je n'ai aucune idée.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4446</th>\n",
       "      <td>Are you home?</td>\n",
       "      <td>Êtes-vous chez vous ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                          tar\n",
       "33710     Yeah, you're right.     Ouais, vous avez raison.\n",
       "777                 Taste it.                    Goûte-la.\n",
       "43841   I'm your best friend.  Je suis votre meilleur ami.\n",
       "5156            I work alone.          Je travaille seule.\n",
       "22368       You're fortunate.         Vous êtes chanceuse.\n",
       "18430       I am pretty sure.    Je suis relativement sûr.\n",
       "41100   Do you need the keys?      T'as besoin des clefs ?\n",
       "48391  Don't throw that away.            Ne le jetez pas !\n",
       "5406            I've no idea.         Je n'ai aucune idée.\n",
       "4446            Are you home?        Êtes-vous chez vous ?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000]  # 6만개만 저장\n",
    "lines.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20969</th>\n",
       "      <td>Tom clearly lied.</td>\n",
       "      <td>\\t Tom a manifestement menti. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31179</th>\n",
       "      <td>Keep your hands up.</td>\n",
       "      <td>\\t Garde les mains en l'air ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53345</th>\n",
       "      <td>We can't see anything.</td>\n",
       "      <td>\\t Nous n'arrivons à rien voir. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26303</th>\n",
       "      <td>They'll like that.</td>\n",
       "      <td>\\t Ils vont aimer cela. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33557</th>\n",
       "      <td>Which team is ours?</td>\n",
       "      <td>\\t Quelle équipe est la nôtre ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31621</th>\n",
       "      <td>She made me a cake.</td>\n",
       "      <td>\\t Elle me concocta un gâteau. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57491</th>\n",
       "      <td>I went straight to bed.</td>\n",
       "      <td>\\t Je suis allée directement au lit. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55643</th>\n",
       "      <td>Have you been drinking?</td>\n",
       "      <td>\\t Avez-vous bu ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22649</th>\n",
       "      <td>Are you ready now?</td>\n",
       "      <td>\\t Es-tu prête maintenant ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31969</th>\n",
       "      <td>The door blew shut.</td>\n",
       "      <td>\\t La porte claqua dans un souffle. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                      tar\n",
       "20969        Tom clearly lied.         \\t Tom a manifestement menti. \\n\n",
       "31179      Keep your hands up.         \\t Garde les mains en l'air ! \\n\n",
       "53345   We can't see anything.       \\t Nous n'arrivons à rien voir. \\n\n",
       "26303       They'll like that.               \\t Ils vont aimer cela. \\n\n",
       "33557      Which team is ours?       \\t Quelle équipe est la nôtre ? \\n\n",
       "31621      She made me a cake.        \\t Elle me concocta un gâteau. \\n\n",
       "57491  I went straight to bed.  \\t Je suis allée directement au lit. \\n\n",
       "55643  Have you been drinking?                     \\t Avez-vous bu ? \\n\n",
       "22649       Are you ready now?           \\t Es-tu prête maintenant ? \\n\n",
       "31969      The door blew shut.   \\t La porte claqua dans un souffle. \\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tar(프랑스어 데이터)는 <sos>와 <eos>를 넣어주어야 한다. \n",
    "# <sos> -> \\t\n",
    "# <eos> -> \\n\n",
    "lines.tar = lines.tar.apply(lambda x: '\\t ' + x + ' \\n')\n",
    "lines.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "    for char in line: # char: 한 문자\n",
    "        src_vocab.add(char)\n",
    "        \n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 79\n",
      "target 문장의 char 집합 : 105\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print('source 문장의 char 집합 :',src_vocab_size)\n",
    "print('target 문장의 char 집합 :',tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n"
     ]
    }
   ],
   "source": [
    "# 각 문자에 인덱스를 부여\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "# 인덱스가 부여된 문자 집합을 가지고 훈련 데이터에 정수 인코딩을 수행\n",
    "encoder_input = []\n",
    "\n",
    "# 1개의 문장\n",
    "for line in lines.src:\n",
    "  encoded_line = []\n",
    "  # 각 줄에서 1개의 char\n",
    "  for char in line:\n",
    "    # 각 char을 정수로 변환\n",
    "    encoded_line.append(src_to_index[char])\n",
    "  encoder_input.append(encoded_line)\n",
    "print('source 문장의 정수 인코딩 :',encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장의 정수 인코딩 : [[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [1, 3, 31, 66, 3, 70, 67, 73, 72, 57, 3, 4, 3, 2], [1, 3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 프랑스어 데이터에 대해서 정수 인코딩을 수행\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    encoded_line.append(tar_to_index[char])\n",
    "  decoder_input.append(encoded_line)\n",
    "print('target 문장의 정수 인코딩 :',decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장 레이블의 정수 인코딩 : [[3, 48, 53, 3, 4, 3, 2], [3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [3, 31, 66, 3, 70, 67, 73, 72, 57, 3, 4, 3, 2], [3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 예측값과 비교하기 위한 실제값 정수 인코딩도 필요\n",
    "# 실제값은 앞에 \\t가 없으므로 제거해준 상태로 정수 인코딩이 필요\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "  timestep = 0\n",
    "  encoded_line = []\n",
    "  for char in line:\n",
    "    if timestep > 0:\n",
    "      encoded_line.append(tar_to_index[char])\n",
    "    timestep = timestep + 1\n",
    "  decoder_target.append(encoded_line)\n",
    "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 최대 길이 : 23\n",
      "target 문장의 최대 길이 : 76\n"
     ]
    }
   ],
   "source": [
    "# 문장 길이 맞추기\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print('source 문장의 최대 길이 :',max_src_len)\n",
    "print('target 문장의 최대 길이 :',max_tar_len)\n",
    "\n",
    "encoder_input = pad_sequences(\n",
    "    encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(\n",
    "    decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(\n",
    "    decoder_target, maxlen=max_tar_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]]\n",
      "\n",
      "\n",
      " [[[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]]\n",
      "\n",
      "\n",
      " [[[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]]\n",
      "\n",
      "\n",
      " [[[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]]\n",
      "\n",
      "\n",
      " [[[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   ...\n",
      "   [1. 0.]\n",
      "   [1. 0.]\n",
      "   [1. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "# 모든 값에 대해서 원-핫 인코딩 수행\n",
    "# 문자 단위 번역기므로 워드 임베딩은 별도로 사용되지 않음\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교사 강요\n",
    "# 훈련 과정에서 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣지 X\n",
    "# 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 사용\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "\n",
    "# encoder_outputs은 여기서는 불필요\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태. -> 이 두가지가 디코더에 전달됨\n",
    "encoder_states = [state_h, state_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "    decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 79) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 79), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 23, 79, 2).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 105) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 105), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 76, 105, 2).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model\" (type Functional).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 23, 79, 2)\n    \n    Call arguments received by layer \"model\" (type Functional):\n      • inputs=('tf.Tensor(shape=(None, 23, 79, 2), dtype=float32)', 'tf.Tensor(shape=(None, 76, 105, 2), dtype=float32)')\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\STUDY\\방학(4)\\Prometheus-ML\\basic_wk10\\ch_14.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/STUDY/%EB%B0%A9%ED%95%99%284%29/Prometheus-ML/basic_wk10/ch_14.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 훈련 과정에서는 인코더 입력, 디코더 입력, 디코더의 실제값인 decoder_target도 필요\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/STUDY/%EB%B0%A9%ED%95%99%284%29/Prometheus-ML/basic_wk10/ch_14.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m[encoder_input, decoder_input], y\u001b[39m=\u001b[39;49mdecoder_target,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/STUDY/%EB%B0%A9%ED%95%99%284%29/Prometheus-ML/basic_wk10/ch_14.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m           batch_size\u001b[39m=\u001b[39;49m\u001b[39m79\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3tl2v_s1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\sksoh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model\" (type Functional).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 23, 79, 2)\n    \n    Call arguments received by layer \"model\" (type Functional):\n      • inputs=('tf.Tensor(shape=(None, 23, 79, 2), dtype=float32)', 'tf.Tensor(shape=(None, 76, 105, 2), dtype=float32)')\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# 훈련 과정에서는 인코더 입력, 디코더 입력, 디코더의 실제값인 decoder_target도 필요\n",
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target,\n",
    "          batch_size=64, epochs=40, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
    "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                      outputs=[decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스로부터 단어 만들기\n",
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000 # 개수 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수들을 구현(구두점 등을 제거, 단어와 구분)\n",
    "def to_ascii(s):\n",
    "      # 프랑스어 악센트(accent) 삭제\n",
    "  # 예시 : 'déjà diné' -> deja dine\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "  # 악센트 제거 함수 호출\n",
    "  sent = to_ascii(sent.lower())\n",
    "\n",
    "  # 단어와 구두점 사이에 공백 추가.\n",
    "  # ex) \"I am a student.\" => \"I am a student .\"\n",
    "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "  # 다수 개의 공백을 하나의 공백으로 치환\n",
    "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "  return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :', preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 출력 시퀀스에 <sos>, <eos> 붙히기\n",
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"./fra-eng/fra.txt\", \"r\", encoding='UTF8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합 생성, 정수 인코딩, 패딩 진행\n",
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 8)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4672, 프랑스어 단어 집합의 크기 : 8137\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(\n",
    "    src_vocab_size, tar_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [24498 26881 22400 ... 26554 12519 18904]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 섞기\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 분리\n",
    "n_of_val = int(33000*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 8)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 8)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_units = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs)  # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)  # 패딩 0은 연산에서 제외\n",
    "# 상태값 리턴을 위해 return_state는 True\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(\n",
    "    enc_masking)  # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c]  # 인코더의 은닉 상태와 셀 상태를 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units)  # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs)  # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 모델의 입력과 출력을 정의.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 단계 인코더\n",
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "    dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "      # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # <SOS>에 해당하는 정수 생성\n",
    "  target_seq = np.zeros((1,1))\n",
    "  target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "  stop_condition = False\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  # stop_condition이 True가 될 때까지 루프 반복\n",
    "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "  while not stop_condition:\n",
    "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # 예측 결과를 단어로 변환\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "    decoded_sentence += ' '+sampled_char\n",
    "\n",
    "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "    if (sampled_char == '<eos>' or\n",
    "        len(decoded_sentence) > 50):\n",
    "        stop_condition = True\n",
    "\n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "\n",
    "\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "          sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "  return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 371ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "입력문장 : no one can get in . \n",
      "정답문장 : personne ne peut y entrer . \n",
      "번역문장 : attrapes malheureux partiale peuvent partiale\n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "입력문장 : how big you are ! \n",
      "정답문장 : que vous etes grand ! \n",
      "번역문장 : attrapes savon remuez toyota savon laver estomac refl\n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "입력문장 : tom s home . \n",
      "정답문장 : tom est a la maison . \n",
      "번역문장 : attrapes partiale payait payait mignonne photogr\n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "입력문장 : i made her a dress . \n",
      "정답문장 : je lui ai confectionne une robe . \n",
      "번역문장 : idolatrait pigez rome fixe dix astucieux envieuse \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "입력문장 : they re in danger . \n",
      "정답문장 : ils sont en danger . \n",
      "번역문장 : attrapes partiale payait payait mignonne paradis photogr\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 메커니즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq 단점\n",
    "# 1. 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생\n",
    "# 2. 기울기 소실\n",
    "\n",
    "# 어텐션의 아이디어\n",
    "# 디코더에서 출력 단어를 예측하는 매 시점마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 닷-프로덕트 어텐션\n",
    "# 어텐션 스코어 -> 소프트맥스(어텐션 스코어) -> 인코더 은닉 상태 * 소프트맥스(어텐션 스코어) -> 합함 -> 어텐션 값\n",
    "# 어텐션 값 + 디코더 은닉 상태(concentrate) -> 예측"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 바다나우 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 닷-프로턱트에서는 디코더의 t시점의 은닉 상태를 사용했지만 바다나우 어텐션에서는 디코더의 t-1시점의 은닉 상태를 사용\n",
    "# 계산식을 통해서 어텐션 스코어 구함 -> 소프트맥스(어텐션 스코어)를 통해서 어텐션 분포를 구함 -> 각 인코더의 어텐션 가중치와 각 인코더의 은닉 상태를 가중합 -> 어텐션 값(인코더의 문맥을 포함한 컨텍스트 벡터)\n",
    "# 컨텍스트 벡터와 임베딩된 단어 벡터를 연결하여 입력으로 사용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 양방향 LSTM과 어텐션 메커니즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e0334895c7257ffa902a82750dbe09ec85e290cbc1321a3819ebf1c9410545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
