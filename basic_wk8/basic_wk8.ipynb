{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4e9faa0",
      "metadata": {
        "id": "e4e9faa0"
      },
      "source": [
        "# CV란?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0eac1d",
      "metadata": {
        "id": "7e0eac1d"
      },
      "source": [
        "컴퓨터 비전이란 컴퓨터가 세계를 시각적인 측면에서 이해할 수 있도록 학습시키는 인공지능 분야이다. 기본적으로 크게 4가지의 테스크로 나눌 수 있는데, \n",
        "\n",
        "1. **Image Classification**\n",
        "2. **Object Detection**\n",
        "3. **Semantic Segmentation**\n",
        "4. **Instance Segmentation**\n",
        "\n",
        "로 나뉜다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d857ef84",
      "metadata": {
        "id": "d857ef84"
      },
      "source": [
        "<img src=\"http://www.smart-interaction.com/wp/wp-content/uploads/2022/06/71-768x768.png\" alt=\"Alternative text\" style=\"width:400px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f20612",
      "metadata": {
        "id": "d0f20612"
      },
      "source": [
        "* Image Classification은 사진을 분석해서 사진에 있는 물체가 어느 종류에 속하는지 분류하는 Task이다.\n",
        "<br>\n",
        "* Object detection은 사진을 분석해 사진 안에 존재하는 복수의 물체에 대해서 각각을 분류함과 동시에 사진에서의 위치를 판별하는 Task이다.\n",
        "<br>\n",
        "* Semantic Segmentation은 사진 내에서 같은 종류에 속하는 물체의 면적을 정확하게 예측하는 Task이다.\n",
        "<br>\n",
        "* 마지막으로 Instance Segmentation은 Object Detection과 Semantic Segmentation을 합친 것으로, 사진 내에서 같은 종류라 할지라도 개별적인 물체들을 서로 독립적으로 분류하는 Task이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb83c6ab",
      "metadata": {
        "id": "fb83c6ab"
      },
      "source": [
        "<img src=\"https://www.anolytics.ai/wp-content/uploads/2022/07/segment_sgment.jpg\" alt=\"Alternative text\" style=\"width:400px;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44649fa0",
      "metadata": {
        "id": "44649fa0"
      },
      "source": [
        "ps: Semantic Segmentation에서도 다른 종류들은 다른 색깔로 동시에 분류할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15902e14",
      "metadata": {
        "id": "15902e14"
      },
      "source": [
        "매우 복잡하고 정교한 모델처럼 보이지만 그 근간은 아주 간단한 인공지능 레이어가 촘촘히 쌓이고 학습되어 만들어진 것이다. 즉, 기본을 배우면서 분석한다면 충분히 분석하고 만들어 낼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3cb4b5d",
      "metadata": {
        "id": "d3cb4b5d"
      },
      "source": [
        "# CV Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "955d69ac",
      "metadata": {
        "id": "955d69ac"
      },
      "source": [
        "## Linear layer의 한계"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff4f8ce6",
      "metadata": {
        "id": "ff4f8ce6"
      },
      "source": [
        "예전에 튜토리얼로 수행했던 MNIST 예제를 기억할 것이다. MNIST를 분석하기 위해서 우리는 받은 (1x28x28)짜리의 MNIST사진을 Flatten시키고 복수의 Linear층과의 행렬연산을 통해서 값을 도출했다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7be0c6",
      "metadata": {
        "id": "4d7be0c6"
      },
      "source": [
        "MNIST처럼 간단한 예제에서는 그 값을 매우 정확하게 예측할 수 있었다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d768f52d",
      "metadata": {
        "id": "d768f52d"
      },
      "source": [
        "하지만 Linear 층만으로 입체적인 이미지 데이터를 분석하기에는 한계가 존재한다. 아래의 예제를 보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707e37b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "5f8fecbdda1249fa998abb20e5cb27e3",
            "c4bca2c64619436d923914622198f97d",
            "1760f0a207c6489097adac3093770d17",
            "835ebfc2b6db4d77913c2489b87b5276",
            "a3e5c51ee8f144c7a7c3ae170eee96b3",
            "a1ca402ce111498fb8142fa39b4dbbca",
            "a2a49aed5fd44c6da3d8b05988ad7a01",
            "f2bdac9a692541e78268439daab9158c",
            "944553d35cba4b4dbaec95ab96fc273e",
            "068a9845ed69467c90050669675e86b4",
            "ef2e80e27af54baf8f90d7c2882bf0a6"
          ]
        },
        "id": "707e37b5",
        "outputId": "016bf855-6930-4b85-a96a-73295c4c1e9d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f8fecbdda1249fa998abb20e5cb27e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary as summary_\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OfM_mlmdYVYd",
      "metadata": {
        "id": "OfM_mlmdYVYd"
      },
      "source": [
        "CIFAR10 이미지 데이터셋을 DNN을 이용한 인공지능으로 학습하고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4eb8109",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4eb8109",
        "outputId": "711b4cbb-4d75-4192-ee45-0fcceeb23a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                 [-1, 3072]               0\n",
            "            Linear-2                  [-1, 400]       1,229,200\n",
            "              ReLU-3                  [-1, 400]               0\n",
            "            Linear-4                  [-1, 500]         200,500\n",
            "              ReLU-5                  [-1, 500]               0\n",
            "            Linear-6                   [-1, 10]           5,010\n",
            "           Softmax-7                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 1,434,710\n",
            "Trainable params: 1,434,710\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 5.47\n",
            "Estimated Total Size (MB): 5.52\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class FlatModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FlatModel, self).__init__()\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        self.Linear1 = nn.Linear(3*32*32, 400)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.Linear2 = nn.Linear(400, 500)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.Linear3 = nn.Linear(500,10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.flat(x)\n",
        "        \n",
        "        out = self.Linear1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.Linear2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.Linear3(out)\n",
        "        out = self.softmax(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "flatModel = FlatModel()\n",
        "flatModel = flatModel.cuda()\n",
        "\n",
        "summary_(flatModel, (3,32,32))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f14b052",
      "metadata": {
        "id": "2f14b052"
      },
      "source": [
        "벌써 파라미터의 수가 매우 많은 것을 확인 할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e11d4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e11d4b",
        "outputId": "a3353465-b68f-41c3-dca0-5f8f07fbf520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch : 0 [781/782]| loss: 2.299 | acc: 17.542\n",
            "train epoch : 1 [781/782]| loss: 2.282 | acc: 19.534\n",
            "train epoch : 2 [781/782]| loss: 2.243 | acc: 21.696\n",
            "train epoch : 3 [781/782]| loss: 2.212 | acc: 23.462\n",
            "train epoch : 4 [781/782]| loss: 2.195 | acc: 25.326\n",
            "train epoch : 5 [781/782]| loss: 2.180 | acc: 27.430\n",
            "train epoch : 6 [781/782]| loss: 2.166 | acc: 29.338\n",
            "train epoch : 7 [781/782]| loss: 2.153 | acc: 30.996\n",
            "train epoch : 8 [781/782]| loss: 2.140 | acc: 32.442\n",
            "train epoch : 9 [781/782]| loss: 2.126 | acc: 34.016\n",
            "end of training\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(flatModel.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train(model, epoch, device):\n",
        "    model.train()\n",
        "    \n",
        "    for i in range(epoch):\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for idx, (input, target) in enumerate(trainloader):\n",
        "            \n",
        "            \n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
        "        i, idx, len(trainloader), train_loss/(idx+1), acc))\n",
        "    \n",
        "    print(\"end of training\")\n",
        "    \n",
        "\n",
        "train(flatModel, 10, 'cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01CG-nC_7yu9",
      "metadata": {
        "id": "01CG-nC_7yu9"
      },
      "source": [
        "Linear레이어를 이용한 간단한 모델을 제작해서 10번의 에폭을 거쳤음에도 눈에 띄는 변화가 일어나지 않았다. 1차원 흑백 사진이었던 MNIST와는 다르게 CIFAR10은 컬러 이미지로 3x32x32의 고차원 정보를 담고 있어 Linear 레이어 만으로 모든 정보를 제대로 학습하기가 쉽지 않아서도 있지만, Linear레이어의 구조적인 문제점 또한 존재한다. CIFAR10 데이터를 활용할 때 기존의 Linear 방식의 문제점을 알 수 있는데, 바로 컬러 이미지를 위치, 색깔에 관계없이 무조건 Flatten을 한 뒤 완전 연결시켜 행렬연산을 한다는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "odJYkJPy87h6",
      "metadata": {
        "id": "odJYkJPy87h6"
      },
      "source": [
        "<img src=\"https://cdn.crowdpic.net/detail-thumb/thumb_d_EFB9A72049730D626BB3A7CFF4C08AF5.jpeg\" alt=\"Alternative text\" style=\"width:300px;\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dDyIwoJ297C0",
      "metadata": {
        "id": "dDyIwoJ297C0"
      },
      "source": [
        "우리는 사물을 인식할 때 해당 사물의 경계(색 변화)나 사물의 모양새 등 공간적 정보를 이용한다. 하지만 이와 다르게 Linear 층은 모든 경계를 무너뜨리고, 각 구역별 연산도 수행하지 않아 이러한 공간적 정보를 활용하지 못한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qRztk-d5-et2",
      "metadata": {
        "id": "qRztk-d5-et2"
      },
      "source": [
        "공간적 정보를 인공지능에게 학습하기 위해 인간의 눈을 모방해서 구조가 바로 우리가 오늘 배울 CNN(Convolution Neural Network)이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r2_H4XdP_ru8",
      "metadata": {
        "id": "r2_H4XdP_ru8"
      },
      "source": [
        "## CNN에 대해서"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mZf7Pz06_ts_",
      "metadata": {
        "id": "mZf7Pz06_ts_"
      },
      "source": [
        "CNN에 대해서 간단하게 이해해보자. CNN에 대한 설명은 정말로 많이 있으니 여기서 이해가 안 돼도 인터넷을 찾아보도록하자.\n",
        "<br>\n",
        "참고로 필자는 쉽게 이해하려면 https://youngq.tistory.com/40 를 추천한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6qwQyGTzZWpU",
      "metadata": {
        "id": "6qwQyGTzZWpU"
      },
      "source": [
        "CNN의 구조는 크게 아래의 구조를 띈다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dlb7JgBnYwg4",
      "metadata": {
        "id": "Dlb7JgBnYwg4"
      },
      "source": [
        "<img src=\"https://taewanmerepo.github.io/2018/01/cnn/head.png\" alt=\"Alternative text\" style=\"width:300ㅔx;\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8SZrBC63Zjro",
      "metadata": {
        "id": "8SZrBC63Zjro"
      },
      "source": [
        "Input을 이용해서 공간적 정보들을 추출, 압축하는 Feature extraction 단계와 그 정보들을 이용하는 우리가 일반적으로 DNN에서 수행하는 Classification 단계가 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lu_I4FONZ6w5",
      "metadata": {
        "id": "Lu_I4FONZ6w5"
      },
      "source": [
        "### Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NbAn8G7BZ-g0",
      "metadata": {
        "id": "NbAn8G7BZ-g0"
      },
      "source": [
        "Feature extraction은 공간적 정보를 압축하기 위해 Convolution 계층과 Pooling 계층을 이용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdyvk-jraK6M",
      "metadata": {
        "id": "cdyvk-jraK6M"
      },
      "source": [
        "<img src=\"https://ifh.cc/g/N5dJmg.png\" alt=\"Alternative text\" width=\"250\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yv0jirv9bRfx",
      "metadata": {
        "id": "Yv0jirv9bRfx"
      },
      "source": [
        "이 숫자 2가 담긴 이미지를 input된 이미지라고 하자. 이 사진은 겉으로 보기에는 연속된 선으로 이루어진 숫자처럼 보이지만 사실 매우 작은 픽셀단위로 나뉘어져 있으며, 각 픽셀에는 밝기를 나타내는 값이 저장되어 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PKKKjgUqdhlv",
      "metadata": {
        "id": "PKKKjgUqdhlv"
      },
      "source": [
        "<img src=\"https://ifh.cc/g/gvCSm5.png\" alt=\"Alternative text\" width=\"300\"/>\n",
        "\n",
        "<img src=\"https://ifh.cc/g/g0CQMz.jpg\" alt=\"Alternative text\" width=\"300\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KdG9KwfEeBd6",
      "metadata": {
        "id": "KdG9KwfEeBd6"
      },
      "source": [
        "Convolution은 이러한 공간적으로 밀접한 픽셀들의 정보를 더 잘 함축하기 위해 고안된 계층으로 구역(kernel)별로 나누어서 해당 구역 내에 같이 존재하는 픽셀별로 연산을 수행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZqpwouwgfK2j",
      "metadata": {
        "id": "ZqpwouwgfK2j"
      },
      "source": [
        "<img src=\"https://taewanmerepo.github.io/2018/01/cnn/conv.png\" alt=\"Alternative text\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LUJQ05hCfO31",
      "metadata": {
        "id": "LUJQ05hCfO31"
      },
      "source": [
        "Kernel_size 만큼의 크기로 정의된 Filter가 Input 위를 움직이며 해당 구역에 포함된 픽셀들을 필터와 곱한 다음 합하게 되고, 그 값을 output에 해당하는 Feature Map에 저장하게 된다"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o2Rb1q14e6G_",
      "metadata": {
        "id": "o2Rb1q14e6G_"
      },
      "source": [
        "<img src=\"https://taewanmerepo.github.io/2018/01/cnn/filter.jpg\" alt=\"Alternative text\" width=\"500\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1Ej5lMuCfekU",
      "metadata": {
        "id": "1Ej5lMuCfekU"
      },
      "source": [
        "이때, Filter의 수는 여러 개일 수 있으며, 각 필터들의 연산으로 나온 값은 각각 다른 Feature Map에 저장된다.\n",
        "<br>\n",
        "즉, Filter의 수만큼 output인 Feature Map의 수가 정의된다는 것이다"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S8B2_a1qeYmV",
      "metadata": {
        "id": "S8B2_a1qeYmV"
      },
      "source": [
        "<img src=\"https://ifh.cc/g/6k0ofS.jpg\" alt=\"Alternative text\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fBI-zPce1G4",
      "metadata": {
        "id": "4fBI-zPce1G4"
      },
      "source": [
        "그런데 위에 사진에서 처럼 kernel_size로 인해서 Feature Map의 크기는 항상 원본인 Input보다 작다는 것을 알 수 있다. 이는 Convolution층의 문제인데, 바로 층을 깊게 만들수록 Feature Map이 점점 작아져 일반적인 방법으로는 층의 깊이를 키우는데 한계가 있다는 것이다.\n",
        "<br>\n",
        "그래서 Convolution layer에서는 이를 해결하기 위해서 **Padding**이라는 것을 사용한다"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qgn3gU4AkC35",
      "metadata": {
        "id": "Qgn3gU4AkC35"
      },
      "source": [
        "<img src=\"https://taewanmerepo.github.io/2018/01/cnn/padding.png\" alt=\"Alternative text\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eJAftwIdkmvd",
      "metadata": {
        "id": "eJAftwIdkmvd"
      },
      "source": [
        "이렇게 input data의 가장자리에 0을 추가해서 이미지를 실제보다 크게 만드는 것이다. 이렇게 한다면 kernel_size로 인해서 감소하는 Feature Map의 크기와 padding으로 인해 늘어난 Feature Map의 크기가 상쇄되어 Input과 Feature Map의 크기를 일정하게 유지할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WZWMmWgwnYuC",
      "metadata": {
        "id": "WZWMmWgwnYuC"
      },
      "source": [
        "하지만, 패딩을 항상 사용하지는 않는다. 맨 위 CNN의 구조에서 볼 수 있듯이, CNN은 공간적 정보를 압축하며 Map의 크기를 줄이는 대신, Channel의 수를 늘려 정보를 저장한다. 그래서 의도적으로 Map의 크기를 줄이기도 한다.\n",
        "<br>\n",
        "이때, CNN 같은 가중치를 이용한 압축이 아닌, Pooling을 통한 압축을 수행할 때도 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frCUgRg7oSgk",
      "metadata": {
        "id": "frCUgRg7oSgk"
      },
      "source": [
        "<img src=\"https://taewanmerepo.github.io/2018/02/cnn/maxpulling.png\" alt=\"Alternative text\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jcgCSgDIohgh",
      "metadata": {
        "id": "jcgCSgDIohgh"
      },
      "source": [
        "이처럼 구역 내의 최댓값 또는 평균을 통해 새로운 Map을 만들어 압축하는 경우가 있다.\n",
        "<br>\n",
        "**이때, Pooling layer에서는 feature map의 증가는 일어나지 않는다**. 단순히 크기를 줄일 뿐이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0KXnjhjbpYG-",
      "metadata": {
        "id": "0KXnjhjbpYG-"
      },
      "source": [
        "### CNN의 구조"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RRlvSPPFpuCo",
      "metadata": {
        "id": "RRlvSPPFpuCo"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\" alt=\"Alternative text\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lF1FAzsypxjb",
      "metadata": {
        "id": "lF1FAzsypxjb"
      },
      "source": [
        "다시 해당 CNN의 구조를 보면 Convolution layer를 거치면서 feature map의 크기가 줄어들고 그 수는 늘어났고, Pooling 과정에서는 크기만 줄어드는 것을 확인할 수 있다.\n",
        "<br>\n",
        "이렇게 압축을 거치면서 인공지능은 이미지가 갖는 공간적 정보(선, 원, 경계 등)에 대해서 학습하며 DNN보다 효과적으로 학습을 할 수 있는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jYJXz8MAozpa",
      "metadata": {
        "id": "jYJXz8MAozpa"
      },
      "source": [
        "### Convolution pytorch documentation 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FFBjGWsQpLDM",
      "metadata": {
        "id": "FFBjGWsQpLDM"
      },
      "source": [
        "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "<br>\n",
        "해당 문서는 covolution layer에 해당하는 torch.nn.Conv2d 클래스에 대해서 설명하고 있다. 읽어본다면 직접 코딩할 때 많은 도움이 될 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oFWdKlOFq-63",
      "metadata": {
        "id": "oFWdKlOFq-63"
      },
      "source": [
        "<p>1. stride : Filter가 한번에 움직이는 길이이다. 일반적으로는 1로 설정되어 있지만 여러 목적으로 더 커질 수 있다. 해당 예시에서는 2</p>\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbn02Ql%2Fbtq1Nvc2Ywd%2FKEzmwiYmIOZSYSj2ESb4ZK%2Fimg.png\" alt=\"Alternative text\" width=\"400\" />\n",
        "<br>\n",
        "<p>2. in_channels, out_channels : input의 map 수와 output인 feature map의 수(Filter의 수)를 정의하는 부분</p>\n",
        "<br>\n",
        "<p>3. kernel_size : Filter의 크기</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1TYO5e8pr3Cf",
      "metadata": {
        "id": "1TYO5e8pr3Cf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4_qzfKYhAD9C",
      "metadata": {
        "id": "4_qzfKYhAD9C"
      },
      "source": [
        "## CNN 구조 코딩"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b58N99g0dtwz",
      "metadata": {
        "id": "b58N99g0dtwz"
      },
      "source": [
        "동일한 CIFAR10 데이터셋을 학습시켜 CNN 모델의 강력함을 알아보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HIDYiDJl-yel",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIDYiDJl-yel",
        "outputId": "a1f59d60-7c7f-4327-d4ce-6ed445abe0bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 16, 16]             168\n",
            "              ReLU-2            [-1, 6, 16, 16]               0\n",
            "            Conv2d-3            [-1, 6, 16, 16]             330\n",
            "              ReLU-4            [-1, 6, 16, 16]               0\n",
            "            Conv2d-5             [-1, 12, 8, 8]             660\n",
            "              ReLU-6             [-1, 12, 8, 8]               0\n",
            "            Conv2d-7             [-1, 12, 8, 8]           1,308\n",
            "           Flatten-8                  [-1, 768]               0\n",
            "            Linear-9                  [-1, 100]          76,900\n",
            "           Linear-10                   [-1, 10]           1,010\n",
            "================================================================\n",
            "Total params: 80,376\n",
            "Trainable params: 80,376\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.07\n",
            "Params size (MB): 0.31\n",
            "Estimated Total Size (MB): 0.39\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=3, stride=2, padding = 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(6, 6, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(6, 12, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(12, 12, kernel_size=3, stride=1,padding=1)\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        self.dense1 = nn.Linear(8*8*12, 100)\n",
        "        self.dense2 = nn.Linear(100,10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv4(out)\n",
        "\n",
        "        out = self.flat(out)\n",
        "        out = self.dense1(out)\n",
        "        out = self.dense2(out)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "NewModel = CNNModel()\n",
        "NewModel = NewModel.cuda()\n",
        "\n",
        "summary_(NewModel, (3,32,32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7xkBDTQVCTBW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xkBDTQVCTBW",
        "outputId": "e008646d-7b49-4167-dbff-b2b0cd54ac51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch : 0 [781/782]| loss: 2.303 | acc: 12.066\n",
            "train epoch : 1 [781/782]| loss: 2.255 | acc: 15.902\n",
            "train epoch : 2 [781/782]| loss: 2.032 | acc: 25.622\n",
            "train epoch : 3 [781/782]| loss: 1.962 | acc: 28.754\n",
            "train epoch : 4 [781/782]| loss: 1.857 | acc: 33.884\n",
            "train epoch : 5 [781/782]| loss: 1.767 | acc: 37.626\n",
            "train epoch : 6 [781/782]| loss: 1.705 | acc: 39.554\n",
            "train epoch : 7 [781/782]| loss: 1.642 | acc: 41.626\n",
            "train epoch : 8 [781/782]| loss: 1.585 | acc: 43.212\n",
            "train epoch : 9 [781/782]| loss: 1.532 | acc: 45.254\n",
            "end of training\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(NewModel.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train(model, epoch, device):\n",
        "    model.train()\n",
        "    \n",
        "    for i in range(epoch):\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for idx, (input, target) in enumerate(trainloader):\n",
        "            \n",
        "            \n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
        "        i, idx, len(trainloader), train_loss/(idx+1), acc))\n",
        "    \n",
        "    print(\"end of training\")\n",
        "    \n",
        "\n",
        "train(NewModel, 10, 'cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oeJXRJH3Dfh-",
      "metadata": {
        "id": "oeJXRJH3Dfh-"
      },
      "source": [
        "DNN 보다 훨씬 더 가볍고 학습도 빠르면서 정확도도 높은 것을 확인해볼 수 있다.\n",
        "\n",
        "conv를 더 촘촘히 쌓는다면 더더욱 높은 정확도를 얻을 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Go2Hn55fsyg_",
      "metadata": {
        "id": "Go2Hn55fsyg_"
      },
      "source": [
        "이제 wk8 실습에서 직접 코딩을 해보도록 하자"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "e4e9faa0"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "35e0334895c7257ffa902a82750dbe09ec85e290cbc1321a3819ebf1c9410545"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "068a9845ed69467c90050669675e86b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1760f0a207c6489097adac3093770d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bdac9a692541e78268439daab9158c",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_944553d35cba4b4dbaec95ab96fc273e",
            "value": 170498071
          }
        },
        "5f8fecbdda1249fa998abb20e5cb27e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4bca2c64619436d923914622198f97d",
              "IPY_MODEL_1760f0a207c6489097adac3093770d17",
              "IPY_MODEL_835ebfc2b6db4d77913c2489b87b5276"
            ],
            "layout": "IPY_MODEL_a3e5c51ee8f144c7a7c3ae170eee96b3"
          }
        },
        "835ebfc2b6db4d77913c2489b87b5276": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_068a9845ed69467c90050669675e86b4",
            "placeholder": "​",
            "style": "IPY_MODEL_ef2e80e27af54baf8f90d7c2882bf0a6",
            "value": " 170498071/170498071 [00:03&lt;00:00, 54411436.91it/s]"
          }
        },
        "944553d35cba4b4dbaec95ab96fc273e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1ca402ce111498fb8142fa39b4dbbca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a49aed5fd44c6da3d8b05988ad7a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3e5c51ee8f144c7a7c3ae170eee96b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4bca2c64619436d923914622198f97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1ca402ce111498fb8142fa39b4dbbca",
            "placeholder": "​",
            "style": "IPY_MODEL_a2a49aed5fd44c6da3d8b05988ad7a01",
            "value": "100%"
          }
        },
        "ef2e80e27af54baf8f90d7c2882bf0a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2bdac9a692541e78268439daab9158c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
